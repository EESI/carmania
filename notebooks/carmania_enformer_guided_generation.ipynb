{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "76d64f2d",
      "metadata": {
        "id": "76d64f2d"
      },
      "source": [
        "# CARMANIA + Enformer: Chromatin-Guided Sequence Generation\n",
        "\n",
        "This notebook builds a simple pipeline where:\n",
        "\n",
        "- **CARMANIA** is used as a genomic language model to generate candidate DNA sequences from a short input prompt.\n",
        "- **Enformer** is used as an oracle to predict chromatin-related signals (e.g., accessibility and other regulatory tracks) along each candidate sequence.\n",
        "- A **scalar score** is computed from Enformer’s output (by averaging predicted signal near the center of the sequence), and used to rank the generated sequences.\n",
        "\n",
        "In other words, CARMANIA proposes DNA sequences, and Enformer provides a regulatory-style score so we can preferentially keep sequences that are predicted to have higher central chromatin “openness.”\n",
        "\n",
        "> **Note:** Both models are large. Use a GPU runtime (e.g., Google Colab GPU).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a09a8f0c",
      "metadata": {
        "id": "a09a8f0c"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q enformer-pytorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d70ef8",
      "metadata": {
        "id": "70d70ef8"
      },
      "source": [
        "## Imports & Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc4e1e74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc4e1e74",
        "outputId": "c6054131-731d-46ea-bcf6-6e7689baf6cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from enformer_pytorch import from_pretrained as enformer_from_pretrained, seq_indices_to_one_hot\n",
        "\n",
        "from typing import List, Tuple\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "424a7324",
      "metadata": {
        "id": "424a7324"
      },
      "source": [
        "## Load CARMANIA (DNA LM)\n",
        "\n",
        "Model: `MsAlEhR/carmania-160k-seqlen-human`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1c158681",
      "metadata": {
        "id": "1c158681"
      },
      "outputs": [],
      "source": [
        "\n",
        "CARMANIA_MODEL_NAME = \"MsAlEhR/carmania-160k-seqlen-human\"\n",
        "\n",
        "# CARMANIA README recommends AutoModel + trust_remote_code=True\n",
        "# The custom class inherits from PreTrainedModel, so .generate() should be available.\n",
        "carmania = AutoModel.from_pretrained(\n",
        "    CARMANIA_MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
        ").to(DEVICE)\n",
        "\n",
        "carmania_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    CARMANIA_MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    model_max_length=160_000,\n",
        ")\n",
        "\n",
        "print(\"Loaded CARMANIA.\")\n",
        "\n",
        "# quick sanity check: tokenize a tiny sequence\n",
        "test_inputs = carmania_tokenizer(\"ACGTAGGCTA\", return_tensors=\"pt\").to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    test_outputs = carmania(**test_inputs)\n",
        "print(\"CARMANIA forward pass OK, type:\", type(test_outputs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78fa56e2",
      "metadata": {
        "id": "78fa56e2"
      },
      "source": [
        "## Load Enformer (oracle)\n",
        "\n",
        "We use `EleutherAI/enformer-official-rough` via `enformer-pytorch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0b29ea43",
      "metadata": {
        "id": "0b29ea43"
      },
      "outputs": [],
      "source": [
        "\n",
        "enformer = enformer_from_pretrained(\n",
        "    \"EleutherAI/enformer-official-rough\"\n",
        ").to(DEVICE)\n",
        "\n",
        "enformer.eval()\n",
        "print(\"Loaded Enformer.\")\n",
        "\n",
        "# quick sanity check on dummy indices\n",
        "with torch.no_grad():\n",
        "    dummy_seq = torch.randint(0, 5, (1, 196_608), device=DEVICE)  # A,C,G,T,N indices\n",
        "    dummy_onehot = seq_indices_to_one_hot(dummy_seq)              # (1, 196608, 5)\n",
        "    dummy_out = enformer(dummy_onehot)\n",
        "    print(\"Enformer output keys:\", dummy_out.keys())\n",
        "    print(\"Human head shape:\", dummy_out[\"human\"].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cab9ff9c",
      "metadata": {
        "id": "cab9ff9c"
      },
      "source": [
        "## DNA ⇄ Enformer Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86b8753c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86b8753c",
        "outputId": "49818758-48d7-4599-877a-ea0ddcf20cfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded indices shape: torch.Size([1, 196608])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "DNA_ALPHABET = \"ACGTN\"\n",
        "char_to_idx = {c: i for i, c in enumerate(DNA_ALPHABET)}  # A:0, C:1, G:2, T:3, N:4\n",
        "\n",
        "def dna_to_indices(seq: str) -> torch.Tensor:\n",
        "    \"\"\"Convert a DNA string (A,C,G,T,N) to indices tensor of shape (1, L).\"\"\"\n",
        "    seq = seq.upper()\n",
        "    idxs = [char_to_idx.get(ch, char_to_idx[\"N\"]) for ch in seq]\n",
        "    return torch.tensor([idxs], dtype=torch.long)\n",
        "\n",
        "def center_pad_to_enformer_context(seq: str, context_length: int = 196_608) -> torch.Tensor:\n",
        "    \"\"\"Embed seq into the center of a context_length window with 'N' padding.\"\"\"\n",
        "    L = len(seq)\n",
        "    if L > context_length:\n",
        "        # center crop\n",
        "        start = (L - context_length) // 2\n",
        "        seq = seq[start:start + context_length]\n",
        "        L = len(seq)\n",
        "\n",
        "    pad_total = context_length - L\n",
        "    pad_left = pad_total // 2\n",
        "    pad_right = pad_total - pad_left\n",
        "\n",
        "    padded_seq = \"N\" * pad_left + seq + \"N\" * pad_right\n",
        "    assert len(padded_seq) == context_length\n",
        "\n",
        "    return dna_to_indices(padded_seq)\n",
        "\n",
        "# quick check\n",
        "s = \"ACGT\" * 10\n",
        "idxs = center_pad_to_enformer_context(s)\n",
        "print(\"Padded indices shape:\", idxs.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8947a71",
      "metadata": {
        "id": "c8947a71"
      },
      "source": [
        "## Enformer Chromatin Heuristic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7070ba82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7070ba82",
        "outputId": "82b3e8e7-bbea-4e01-afde-631a2e7163d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Enformer heuristic score: 0.5320137143135071\n"
          ]
        }
      ],
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def enformer_chromatin_open_score(seq: str, center_radius_bins: int = 8) -> float:\n",
        "    \"\"\"\n",
        "    A Sample heuristic:\n",
        "    - Put seq in 196,608 bp context\n",
        "    - Run Enformer (human head)\n",
        "    - Take center +/- center_radius_bins bins\n",
        "    - Average across bins & tracks\n",
        "    Returns: Python float score.\n",
        "    \"\"\"\n",
        "    idxs = center_pad_to_enformer_context(seq)        # (1, 196608)\n",
        "    idxs = idxs.to(DEVICE)\n",
        "    one_hot = seq_indices_to_one_hot(idxs)            # (1, 196608, 5)\n",
        "    one_hot = one_hot.to(DEVICE)\n",
        "\n",
        "    outputs = enformer(one_hot)                       # dict with 'human', 'mouse'\n",
        "    human_pred = outputs[\"human\"]                     # (1, target_len, 5313)\n",
        "\n",
        "    B = human_pred.shape[1]\n",
        "    center = B // 2\n",
        "    start = max(0, center - center_radius_bins)\n",
        "    end = min(B, center + center_radius_bins + 1)\n",
        "\n",
        "    center_slice = human_pred[:, start:end, :]        # (1, bins, tracks)\n",
        "    score = center_slice.mean().item()\n",
        "    return float(score)\n",
        "\n",
        "# quick sanity check\n",
        "test_score = enformer_chromatin_open_score(\"ACGT\" * 50)\n",
        "print(\"Test Enformer heuristic score:\", test_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aee1a940",
      "metadata": {
        "id": "aee1a940"
      },
      "source": [
        "## CARMANIA Sampling Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39cf69cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39cf69cc",
        "outputId": "83a254ad-9cdc-4ec7-91a6-7fde09803688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled sequences: ['ACGTACGTCGGACATCTTGAATAC', 'ACGTACGTTCTACCAATTACTAGT']\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def sample_carmania(\n",
        "    prompt_seq: str,\n",
        "    num_samples: int = 4,\n",
        "    max_new_tokens: int = 64,\n",
        "    temperature: float = 1.0,\n",
        "    top_p: float = 0.95,\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Autoregressive sampler for CARMANIA using its logits (no .generate()).\n",
        "\n",
        "    - Takes a DNA prompt string.\n",
        "    - Encodes with the CARMANIA tokenizer.\n",
        "    - Repeats the prompt `num_samples` times as batch.\n",
        "    - Iteratively samples the next token from logits[:, -1, :].\n",
        "    - Returns decoded sequences (prompt + continuation), restricted to A/C/G/T.\n",
        "    \"\"\"\n",
        "    # 1) Encode the prompt once\n",
        "    enc = carmania_tokenizer(\n",
        "        prompt_seq,\n",
        "        return_tensors=\"pt\",\n",
        "        add_special_tokens=True,\n",
        "    )\n",
        "    input_ids = enc[\"input_ids\"].to(DEVICE)          # (1, L)\n",
        "    # Repeat for batch size = num_samples\n",
        "    input_ids = input_ids.repeat(num_samples, 1)     # (B, L)\n",
        "\n",
        "    # 2) Autoregressive loop\n",
        "    for _ in range(max_new_tokens):\n",
        "        outputs = carmania(input_ids=input_ids)      # CausalLMOutput\n",
        "        logits = outputs.logits                      # (B, seq_len, vocab_size)\n",
        "        next_token_logits = logits[:, -1, :]         # (B, vocab_size)\n",
        "\n",
        "        # Temperature\n",
        "        if temperature is not None and temperature > 0.0:\n",
        "            next_token_logits = next_token_logits / temperature\n",
        "\n",
        "\n",
        "\n",
        "        # Top-p (nucleus) filtering\n",
        "        if top_p is not None and 0.0 < top_p < 1.0:\n",
        "            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "            probs = torch.softmax(sorted_logits, dim=-1)\n",
        "            cumulative_probs = probs.cumsum(dim=-1)\n",
        "\n",
        "            # filter tokens with cumulative prob above top_p\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            # shift right so we always keep at least the first token\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "            # scatter back to original ordering\n",
        "            indices_to_remove = torch.zeros_like(next_token_logits, dtype=torch.bool)\n",
        "            indices_to_remove.scatter_(1, sorted_indices, sorted_indices_to_remove)\n",
        "            next_token_logits = next_token_logits.masked_fill(indices_to_remove, float(\"-inf\"))\n",
        "\n",
        "        # Sample from the filtered distribution\n",
        "        probs = torch.softmax(next_token_logits, dim=-1)\n",
        "        next_tokens = torch.multinomial(probs, num_samples=1)   # (B, 1)\n",
        "\n",
        "        # Append to input_ids\n",
        "        input_ids = torch.cat([input_ids, next_tokens], dim=1)  # (B, L+1)\n",
        "\n",
        "    # 3) Decode all sequences\n",
        "    sequences = carmania_tokenizer.batch_decode(\n",
        "        input_ids,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=True,\n",
        "    )\n",
        "\n",
        "    # 4) Restrict to DNA alphabet\n",
        "    cleaned = []\n",
        "    for s in sequences:\n",
        "        s = s.upper()\n",
        "        s = \"\".join(ch for ch in s if ch in DNA_ALPHABET)\n",
        "        cleaned.append(s)\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "# quick sanity check sampling\n",
        "test_samples = sample_carmania(\"ACGTACGT\", num_samples=2, max_new_tokens=16)\n",
        "print(\"Sampled sequences:\", test_samples)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2c4aacd",
      "metadata": {
        "id": "b2c4aacd"
      },
      "source": [
        "##  Guided Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "778a4c43",
      "metadata": {
        "id": "778a4c43"
      },
      "outputs": [],
      "source": [
        "\n",
        "def guided_beam_search(\n",
        "    prompt_seq: str,\n",
        "    num_beams: int = 4,\n",
        "    fan_out: int = 4,\n",
        "    num_steps: int = 4,\n",
        "    max_new_tokens_per_step: int = 64,\n",
        ") -> List[Tuple[str, float]]:\n",
        "    \"\"\"\n",
        "     beam search:\n",
        "    - CARMANIA as generator\n",
        "    - Enformer as oracle for chromatin accessibility heuristic.\n",
        "    \"\"\"\n",
        "    init_score = enformer_chromatin_open_score(prompt_seq)\n",
        "    beams = [(prompt_seq, init_score)]\n",
        "    print(f\"Initial prompt score: {init_score:.4f}\")\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        print(f\"\\n=== Step {step+1}/{num_steps} ===\")\n",
        "        candidates: List[Tuple[str, float]] = []\n",
        "\n",
        "        for seq, _ in beams:\n",
        "            samples = sample_carmania(\n",
        "                seq,\n",
        "                num_samples=fan_out,\n",
        "                max_new_tokens=max_new_tokens_per_step,\n",
        "            )\n",
        "\n",
        "            for s in samples:\n",
        "                try:\n",
        "                    score = enformer_chromatin_open_score(s)\n",
        "                except RuntimeError as e:\n",
        "                    print(\"Enformer error for candidate, skipping:\", e)\n",
        "                    continue\n",
        "                candidates.append((s, score))\n",
        "\n",
        "        candidates.extend(beams)\n",
        "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "        beams = candidates[:num_beams]\n",
        "\n",
        "        print(\"Top beams this step:\")\n",
        "        for i, (seq, score) in enumerate(beams):\n",
        "            print(f\"  Beam {i+1}: score = {score:.4f}, len = {len(seq)}\")\n",
        "\n",
        "    return beams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dfd55cb",
      "metadata": {
        "id": "9dfd55cb"
      },
      "source": [
        "## Run the Guided Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e341d01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e341d01",
        "outputId": "03d3b2e6-8046-4e06-9b43-6dcfc90ae21f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial prompt score: 0.5407\n",
            "\n",
            "=== Step 1/10 ===\n",
            "Top beams this step:\n",
            "  Beam 1: score = 0.5951, len = 84\n",
            "  Beam 2: score = 0.5407, len = 20\n",
            "  Beam 3: score = 0.5048, len = 84\n",
            "\n",
            "=== Step 2/10 ===\n",
            "Top beams this step:\n",
            "  Beam 1: score = 0.6756, len = 148\n",
            "  Beam 2: score = 0.6117, len = 148\n",
            "  Beam 3: score = 0.5951, len = 84\n",
            "\n",
            "=== Step 3/10 ===\n",
            "Top beams this step:\n",
            "  Beam 1: score = 0.6756, len = 148\n",
            "  Beam 2: score = 0.6516, len = 212\n",
            "  Beam 3: score = 0.6429, len = 212\n",
            "\n",
            "=== Step 4/10 ===\n",
            "Top beams this step:\n",
            "  Beam 1: score = 0.8394, len = 276\n",
            "  Beam 2: score = 0.8118, len = 276\n",
            "  Beam 3: score = 0.7553, len = 276\n",
            "\n",
            "=== Step 5/10 ===\n",
            "Top beams this step:\n",
            "  Beam 1: score = 1.2665, len = 340\n",
            "  Beam 2: score = 1.1864, len = 340\n",
            "  Beam 3: score = 1.1265, len = 340\n",
            "\n",
            "=== Step 6/10 ===\n",
            "Top beams this step:\n",
            "  Beam 1: score = 1.5185, len = 404\n",
            "  Beam 2: score = 1.3278, len = 404\n",
            "  Beam 3: score = 1.2665, len = 340\n",
            "\n",
            "=== Step 7/10 ===\n",
            "Top beams this step:\n",
            "  Beam 1: score = 1.6594, len = 468\n",
            "  Beam 2: score = 1.6505, len = 468\n",
            "  Beam 3: score = 1.5907, len = 468\n",
            "\n",
            "=== Step 8/10 ===\n",
            "Top beams this step:\n",
            "  Beam 1: score = 1.9863, len = 532\n",
            "  Beam 2: score = 1.9681, len = 532\n",
            "  Beam 3: score = 1.9525, len = 532\n",
            "\n",
            "=== Step 9/10 ===\n",
            "Top beams this step:\n",
            "  Beam 1: score = 2.5246, len = 596\n",
            "  Beam 2: score = 2.2741, len = 596\n",
            "  Beam 3: score = 2.1986, len = 596\n",
            "\n",
            "=== Step 10/10 ===\n",
            "Top beams this step:\n",
            "  Beam 1: score = 2.9768, len = 660\n",
            "  Beam 2: score = 2.8865, len = 660\n",
            "  Beam 3: score = 2.5587, len = 660\n",
            "\n",
            "=== Final beams ===\n",
            "\n",
            "Beam 1\n",
            "Score: 2.9768\n",
            "Length: 660\n",
            "ACGTACGTACGTACGTACGTGGAAATGTTCTACGGAAACACAGGGTATGTACTTTCGCTTTGGGTATAGGCGGCATACATTCGTTGAAAAGCGTATACTTTCAAGTCCAAGAACAACCGGTTCTCGGAAATTCTCACGCGGAAAGCACGGTCGCCTTTCGGGGTCTCTCTGTCCCGGGCGCGGTCTGCGGAGCGTACGGCGGGGGAAGCAAAAGGCTTCAGAAGATTCTGTGTGGCGTCAGGAACAAGCCGGCGGAAATACTCAACCGGAGAAAAGGGGCTGGGGAAAAGAATAAGATTC...\n",
            "\n",
            "Beam 2\n",
            "Score: 2.8865\n",
            "Length: 660\n",
            "ACGTACGTACGTACGTACGTGGAAATGTTCTACGGAAACACAGGGTATGTACTTTCGCTTTGGGTATAGGCGGCATACATTCGTTGAAAAGCGTATACTTTCAAGTCCAAGAACAACCGGTTCTCGGAAATTCTCACGCGGAAAGCACGGTCGCCTTTCGGGGTCTCTCTGTCCCGGGCGCGGTCTGCGGAGCGTACGGCGGGGGAAGCAAAAGGCTTCAGAAGATTCTGTGTGGCGTCAGGAACAAGCCGGCGGAAATACTCAACCGGAGAAAAGGGGCTGGGGAAAAGAATAAGATTC...\n",
            "\n",
            "Beam 3\n",
            "Score: 2.5587\n",
            "Length: 660\n",
            "ACGTACGTACGTACGTACGTGGAAATGTTCTACGGAAACACAGGGTATGTACTTTCGCTTTGGGTATAGGCGGCATACATTCGTTGAAAAGCGTATACTTTCAAGTCCAAGAACAACCGGTTCTCGGAAATTCTCACGCGGAAAGCACGGTCGCCTTTCGGGGTCTCTCTGTCCCGGGCGCGGTCTGCGGAGCGTACGGCGGGGGAAGCAAAAGGCTTCAGAAGATTCTGTGTGGCGTCAGGAACAAGCCGGCGGAAATACTCAACCGGAGAAAAGGGGCTGGGGAAAAGAATAAGATTC...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "prompt = \"ACGT\" * 5\n",
        "\n",
        "final_beams = guided_beam_search(\n",
        "    prompt_seq=prompt,\n",
        "    num_beams=3,\n",
        "    fan_out=3,\n",
        "    num_steps=10,\n",
        "    max_new_tokens_per_step=64,\n",
        ")\n",
        "\n",
        "print(\"\\n=== Final beams ===\")\n",
        "for i, (seq, score) in enumerate(final_beams):\n",
        "    print(f\"\\nBeam {i+1}\")\n",
        "    print(f\"Score: {score:.4f}\")\n",
        "    print(f\"Length: {len(seq)}\")\n",
        "    preview = seq[:300] + (\"...\" if len(seq) > 300 else \"\")\n",
        "    print(preview)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above result  shows the top three sequences returned by the chromatin-guided search.  \n",
        "Each “Beam” corresponds to one candidate DNA sequence generated by CARMANIA and then scored by Enformer:\n",
        "\n",
        "- **Score** is the Enformer-based chromatin activity heuristic: higher values indicate stronger predicted regulatory signal in the central region of the sequence.\n",
        "\n",
        "- The sequences share a common prefix from the initial prompt, while the later positions reflect CARMANIA’s sampled continuation that Enformer predicts to be highly active.\n"
      ],
      "metadata": {
        "id": "usknT5cyAFse"
      },
      "id": "usknT5cyAFse"
    },
    {
      "cell_type": "code",
      "source": [
        "s"
      ],
      "metadata": {
        "id": "Qk6GH8PX6rsx"
      },
      "id": "Qk6GH8PX6rsx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}