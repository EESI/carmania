{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a116f31",
   "metadata": {},
   "source": [
    "# CARMANIA Unguided LM Generation \n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. Loads the **CARMANIA** genomic language model.\n",
    "2. Implements **LM scoring** .\n",
    "3. Lets you **generate sequences from a prompt**, rank them by LM score,\n",
    "   and **save the top sequences to a FASTA file**.\n",
    "\n",
    "You only need to:\n",
    "- Set your `prompt` and generation parameters in the last cell.\n",
    "- Run the notebook **top to bottom** in a fresh session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e6b896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CARMANIA and tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "# Device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# Model name\n",
    "CARMANIA_MODEL_NAME = \"MsAlEhR/carmania-160k-seqlen-human\"\n",
    "\n",
    "# Load tokenizer & model\n",
    "carmania = AutoModel.from_pretrained(\n",
    "    CARMANIA_MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    ").to(DEVICE)\n",
    "\n",
    "carmania_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CARMANIA_MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    model_max_length=2000,\n",
    ")\n",
    "carmania.eval()\n",
    "\n",
    "DNA_ALPHABET = set(\"ACGT\")\n",
    "\n",
    "print(\"CARMANIA and tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085e7ad5",
   "metadata": {},
   "source": [
    "## Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0f9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def prepare_batch_carmania(\n",
    "    seqs: List[str],\n",
    "    tokenizer,\n",
    "    device: str = DEVICE,\n",
    "    add_special_tokens: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Tokenize + pad. Builds attention_mask if tokenizer doesn't provide one.\n",
    "    \n",
    "    Returns:\n",
    "      input_ids      [B, L]\n",
    "      attention_mask [B, L] (1 = real token, 0 = pad)\n",
    "      token_lengths  list[int] (number of real tokens per seq)\n",
    "    \"\"\"\n",
    "    enc = tokenizer(\n",
    "        seqs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=False,\n",
    "        add_special_tokens=add_special_tokens,\n",
    "    )\n",
    "\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "\n",
    "    if \"attention_mask\" in enc:\n",
    "        attention_mask = enc[\"attention_mask\"].to(device)\n",
    "    else:\n",
    "        pad_id = tokenizer.pad_token_id\n",
    "        if pad_id is None:\n",
    "            attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=device)\n",
    "        else:\n",
    "            attention_mask = (input_ids != pad_id).long().to(device)\n",
    "\n",
    "    token_lengths = attention_mask.sum(dim=1).tolist()\n",
    "    return input_ids, attention_mask, token_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "987f9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_logprobs_carmania(\n",
    "    logits: torch.Tensor,\n",
    "    input_ids: torch.Tensor,\n",
    "    trim_bos: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Evo-style: log p(x_t | x_<t) per position.\n",
    "\n",
    "    logits: [B, L, V]\n",
    "    input_ids: [B, L]\n",
    "\n",
    "    Returns:\n",
    "      logprobs: [B, L'] where L' = L-1 if trim_bos else L\n",
    "    \"\"\"\n",
    "    softmax_logprobs = torch.log_softmax(logits, dim=-1)   # [B, L, V]\n",
    "\n",
    "    if trim_bos:\n",
    "        # use prediction at pos t-1 to score token at pos t\n",
    "        softmax_logprobs = softmax_logprobs[:, :-1, :]     # [B, L-1, V]\n",
    "        target_ids = input_ids[:, 1:]                      # [B, L-1]\n",
    "    else:\n",
    "        target_ids = input_ids                             # [B, L]\n",
    "\n",
    "    logprobs = torch.gather(\n",
    "        softmax_logprobs,             # [B, L', V]\n",
    "        dim=2,\n",
    "        index=target_ids.unsqueeze(-1)  # [B, L', 1]\n",
    "    ).squeeze(-1)                     # [B, L']\n",
    "\n",
    "    return logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a9679a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sequences_carmania(\n",
    "    seqs: List[str],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    reduce_method: str = \"mean\",\n",
    "    device: str = DEVICE,\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    LM log-likelihood scoring for CARMANIA (Evo-style).\n",
    "\n",
    "    reduce_method:\n",
    "      'mean' -> average log p per token\n",
    "      'sum'  -> total log p\n",
    "    \"\"\"\n",
    "    input_ids, attention_mask, token_lengths = prepare_batch_carmania(\n",
    "        seqs, tokenizer, device=device, add_special_tokens=True\n",
    "    )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # [B, L, V]\n",
    "\n",
    "    # per-position log p(x_t | x_<t)\n",
    "    logprobs = logits_to_logprobs_carmania(\n",
    "        logits,\n",
    "        input_ids,\n",
    "        trim_bos=True,\n",
    "    )  # [B, L-1]\n",
    "\n",
    "    logprobs = logprobs.float().cpu().numpy()\n",
    "\n",
    "    if reduce_method == \"mean\":\n",
    "        reduce_fn = np.mean\n",
    "    elif reduce_method == \"sum\":\n",
    "        reduce_fn = np.sum\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid reduce_method {reduce_method}\")\n",
    "\n",
    "    scores = []\n",
    "    for i, L in enumerate(token_lengths):\n",
    "        effective_len = int(L) - 1  # because we trimmed BOS\n",
    "        seq_logprobs = logprobs[i][:effective_len]\n",
    "        scores.append(reduce_fn(seq_logprobs))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70da95",
   "metadata": {},
   "source": [
    "## Sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e199f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_carmania(\n",
    "    prompt_seq: str,\n",
    "    num_samples: int = 4,\n",
    "    max_new_tokens: int = 64,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.7,\n",
    "):\n",
    "    \"\"\"\n",
    "    Autoregressive sampler for CARMANIA using logits directly (no .generate()).\n",
    "\n",
    "    - Takes a DNA prompt string.\n",
    "    - Repeats it num_samples times as batch.\n",
    "    - Iteratively samples next tokens from logits[:, -1, :].\n",
    "    - Returns:\n",
    "        sequences: cleaned DNA strings (prompt + continuation)\n",
    "        scores: mean log-prob of sampled tokens (per sequence)\n",
    "    \"\"\"\n",
    "    enc = carmania_tokenizer(prompt_seq, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    input_ids = enc[\"input_ids\"].to(DEVICE)\n",
    "    input_ids = input_ids.repeat(num_samples, 1)\n",
    "\n",
    "    step_logprobs = []  # list of [B]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        outputs = carmania(input_ids=input_ids)\n",
    "        logits = outputs.logits              # (B, L, V)\n",
    "\n",
    "        # work in float32 to avoid fp16 overflow issues\n",
    "        next_logits = logits[:, -1, :].float()   # (B, V)\n",
    "\n",
    "        # temperature\n",
    "        if temperature is not None and temperature > 0.0:\n",
    "            next_logits = next_logits / temperature\n",
    "\n",
    "        # top-p (nucleus) sampling\n",
    "        if top_p is not None and 0.0 < top_p < 1.0:\n",
    "            sorted_logits, sorted_idx = torch.sort(next_logits, descending=True)\n",
    "            probs = torch.softmax(sorted_logits, dim=-1)\n",
    "            cdf = probs.cumsum(-1)\n",
    "\n",
    "            mask = cdf > top_p\n",
    "            mask[..., 1:] = mask[..., :-1]\n",
    "            mask[..., 0] = False\n",
    "\n",
    "            rm = torch.zeros_like(next_logits, dtype=torch.bool)\n",
    "            rm.scatter_(1, sorted_idx, mask)\n",
    "            next_logits = next_logits.masked_fill(rm, -1e9)\n",
    "\n",
    "        probs = torch.softmax(next_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, 1)  # (B, 1)\n",
    "\n",
    "        # log-prob of chosen token\n",
    "        step_lp = torch.log(probs.gather(1, next_token).squeeze(1) + 1e-12)  # (B,)\n",
    "        step_logprobs.append(step_lp)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "    # decode sequences\n",
    "    raw_seqs = carmania_tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "    cleaned = []\n",
    "    for s in raw_seqs:\n",
    "        s_up = s.upper()\n",
    "        only_dna = ''.join(c for c in s_up if c in DNA_ALPHABET)\n",
    "        cleaned.append(only_dna if only_dna else s_up)\n",
    "\n",
    "    # mean log-prob per sequence\n",
    "    logprob_tensor = torch.stack(step_logprobs, dim=0)  # (T, B)\n",
    "    scores = logprob_tensor.mean(dim=0).cpu().tolist()\n",
    "\n",
    "    return cleaned, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351cdbb4",
   "metadata": {},
   "source": [
    "## Generate sequences, rank them, and save top ones to FASTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8df89b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_fasta(\n",
    "    prompt: str,\n",
    "    num_samples: int = 32,\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.9,\n",
    "    top_k_to_save: int = 10,\n",
    "    fasta_path: str = \"carmania_top_sequences.fasta\",\n",
    "):\n",
    "    \"\"\"\n",
    "    High-level helper:\n",
    "      1) Sample `num_samples` sequences from CARMANIA given `prompt`.\n",
    "      2) Score all sequences with LM log-likelihood (Evo-style).\n",
    "      3) Rank by LM score (descending).\n",
    "      4) Save top_k_to_save sequences to a FASTA file.\n",
    "\n",
    "    Returns:\n",
    "      ranked: list of (sequence, score) sorted by score desc.\n",
    "      fasta_path: path to the written FASTA file.\n",
    "    \"\"\"\n",
    "    print(f\"Sampling {num_samples} sequences from prompt: {prompt!r}\")\n",
    "    seqs, sample_scores = sample_carmania(\n",
    "        prompt_seq=prompt,\n",
    "        num_samples=num_samples,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    print(\"Scoring sequences with CARMANIA LM...\")\n",
    "    lm_scores = score_sequences_carmania(\n",
    "        seqs,\n",
    "        model=carmania,\n",
    "        tokenizer=carmania_tokenizer,\n",
    "        reduce_method=\"mean\",\n",
    "    )\n",
    "\n",
    "    ranked = sorted(\n",
    "        zip(seqs, lm_scores),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    # Save top K to FASTA\n",
    "    top_k = min(top_k_to_save, len(ranked))\n",
    "    with open(fasta_path, \"w\") as f:\n",
    "        for i in range(top_k):\n",
    "            seq, score = ranked[i]\n",
    "            header = f\">carmania_seq_{i+1}_score_{score:.4f}\"\n",
    "            f.write(header + \"\\n\")\n",
    "            f.write(seq + \"\\n\")\n",
    "\n",
    "    print(f\"Saved top {top_k} sequences to {os.path.abspath(fasta_path)}\")\n",
    "    return ranked, fasta_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b70d7",
   "metadata": {},
   "source": [
    "## Example: run generation + save FASTA for this session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e38df859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 32 sequences from prompt: 'CTTTCTGTCCCGCCCTTCCTCTGACTGTGTCTTGATT'\n",
      "Scoring sequences with CARMANIA LM...\n",
      "Saved top 10 sequences to /scratch/home/sr3622/Firm-DTI/Firm-DTI2/carmania_top_sequences.fasta\n",
      "Rank 1 | score=-0.5021 | length=165\n",
      "CTTTCTGTCCCGCCCTTCCTCTGACTGTGTCTTGATTTTCTTTTTTCTTCTCTTCTCTTCTCTTCTCTTCTCTTCTCTTCTCTTCTCTTCTCTTCTCTTCTCTTCTCTTCTCTTCTCTTCCCTTCCCTTCTCTTCTCTTCCCTTCTCTTCCCTTCCCTTCTCTTC\n",
      "\n",
      "Rank 2 | score=-0.5134 | length=165\n",
      "CTTTCTGTCCCGCCCTTCCTCTGACTGTGTCTTGATTCTTTTTCCCTCCCTCCTTCTCTCTCTCTCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCCCTCC\n",
      "\n",
      "Rank 3 | score=-0.5654 | length=165\n",
      "CTTTCTGTCCCGCCCTTCCTCTGACTGTGTCTTGATTTTTATTTTTATTTTTTTTTGAGATAAAGTCTTGCTCTGTCACCCAGGCTGGAGTGCAGTGGCACGATCTCAGCTCACTGCAACCTCCACCTCCCAGGTTCAAGTGATTCTTGTGCCTCAGCCTCCTGA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# <<< EDIT THESE PARAMETERS AS YOU LIKE >>>\n",
    "prompt = \"CTTTCTGTCCCGCCCTTCCTCTGACTGTGTCTTGATT\"\n",
    "num_samples = 32\n",
    "max_new_tokens = 128\n",
    "temperature = 1.0\n",
    "top_p = 0.9\n",
    "top_k_to_save = 10\n",
    "fasta_path = \"carmania_top_sequences.fasta\"\n",
    "\n",
    "ranked, fasta_file = generate_and_save_fasta(\n",
    "    prompt=prompt,\n",
    "    num_samples=num_samples,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    top_k_to_save=top_k_to_save,\n",
    "    fasta_path=fasta_path,\n",
    ")\n",
    "\n",
    "# Show top 3 in the notebook\n",
    "for i in range(min(3, len(ranked))):\n",
    "    seq, score = ranked[i]\n",
    "    print(f\"Rank {i+1} | score={score:.4f} | length={len(seq)}\")\n",
    "    print(seq[:200] + ('...' if len(seq) > 200 else ''))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa3c667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digress",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
